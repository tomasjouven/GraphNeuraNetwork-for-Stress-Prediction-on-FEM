{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdS0jmMSZyev"
      },
      "source": [
        "## Graph U-Net Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhIQ8EmYZ0UV"
      },
      "source": [
        "###  Drive Mount & Installation\n",
        "\n",
        "We mount Google Drive to access the data and install the required **PyTorch Geometric** libraries for our U-Net GNN implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa5hmND6Yxm4",
        "outputId": "4a17180c-9e39-406a-fd9d-ebf41fdec49b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.0+cu121.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3xeuiglZ4XK"
      },
      "source": [
        "###  Configuration & Hyperparameters\n",
        "\n",
        "This cell generates the `config.py` file, which centralizes all experiment parameters. This allows for quick adjustments to the configuration without modifying the model or training code directly.\n",
        "\n",
        "Key definitions include:\n",
        "* **Data Paths:** Absolute paths to the dataset (mounted via Google Drive).\n",
        "* **GNN Architecture:** Hidden channel dimensions (`HIDDEN_CHANNELS`), depth (`DEPTH`), and pooling ratios.\n",
        "* **Optimization:** Batch size, learning rate, and number of epochs.\n",
        "* **Hardware:** Automatic selection of GPU (CUDA) if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M__GNxNZ5XW",
        "outputId": "36aa23fd-4809-4593-808e-0fbb5ec710c1"
      },
      "outputs": [],
      "source": [
        "%%writefile config.py\n",
        "import torch\n",
        "\n",
        "class Config:\n",
        "    \"\"\"\n",
        "    Central Configuration for the experiment.\n",
        "    All hyperparameters and paths are defined here for easy tuning.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Data Paths (Google Drive) ---\n",
        "    TRAIN_DATA_PATH = '/content/gdrive/MyDrive/DataSetML4/data/train_data.pt'\n",
        "    VAL_DATA_PATH = '/content/gdrive/MyDrive/DataSetML4/data/val_data.pt'\n",
        "\n",
        "    # --- Training Settings ---\n",
        "    TRAIN_SUBSET_RATIO = 1      # Use 1.0 for full dataset, lower (e.g., 0.1) for debugging\n",
        "    BATCH_SIZE = 64             # Number of graphs processed in parallel\n",
        "\n",
        "    # --- Model Architecture (Graph U-Net) ---\n",
        "    IN_CHANNELS = 12            # Input features (position, normals, boundary flags, etc.)\n",
        "    HIDDEN_CHANNELS = 32        # Size of the hidden layers (model capacity)\n",
        "    OUT_CHANNELS = 1            # Output: scalar Von Mises stress\n",
        "    DEPTH = 3                   # Number of U-Net levels (Downsampling/Upsampling steps)\n",
        "    POOL_RATIOS = 0.8           # Keep top 80% of nodes at each pooling step\n",
        "\n",
        "    # --- Optimization ---\n",
        "    NUM_EPOCHS = 2             # Total training iterations\n",
        "    LEARNING_RATE = 0.0001      # Step size for the optimizer\n",
        "    WEIGHT_DECAY = 1e-5         # L2 Regularization to prevent overfitting\n",
        "    GRADIENT_CLIP = 1.0         # Max norm for gradients (stabilizes training)\n",
        "\n",
        "    # --- Saving ---\n",
        "    BEST_MODEL_PATH = 'best_model.pt'\n",
        "    TRAINING_CURVE_PATH = 'training_curve.png'\n",
        "\n",
        "    # --- Hardware ---\n",
        "    # Automatically select GPU if available for faster computation\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx6c4M13crQV"
      },
      "source": [
        "###  Data Loading Compatibility\n",
        "\n",
        "Although the dataset is already normalized, the saved `.pt` files contain instances of `UnitGaussianNormalizer`. We explicitly define this class here to ensure `torch.load` can deserialize the data correctly without raising `AttributeError` or warnings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Cg2lB7KZ8__",
        "outputId": "afbae685-fbb0-4c1e-9f88-f3e278d7f7ed"
      },
      "outputs": [],
      "source": [
        "%%writefile utils.py\n",
        "import torch\n",
        "\n",
        "class UnitGaussianNormalizer:\n",
        "    \"\"\"\n",
        "    Applies Unit Gaussian Normalization (Z-score standardization) to tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x=None, eps=1e-5):\n",
        "        \"\"\"\n",
        "        Calculates and stores the mean and standard deviation of the input data 'x'.\n",
        "        \"\"\"\n",
        "        self.eps = eps\n",
        "        if x is not None:\n",
        "            # Calculate statistics across the 0-th dimension (samples)\n",
        "            self.mean = x.mean(dim=0, keepdim=True)\n",
        "            self.std = x.std(dim=0, keepdim=True) + eps\n",
        "        else:\n",
        "            self.mean = None\n",
        "            self.std = None\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Normalizes the input x (subtract mean, divide by std).\n",
        "        \"\"\"\n",
        "        if self.mean is None:\n",
        "            return x\n",
        "        return (x - self.mean) / self.std\n",
        "\n",
        "    def decode(self, x, sample_idx=None):\n",
        "        \"\"\"\n",
        "        Un-normalizes x (multiply by std, add mean) to recover original units.\n",
        "        \"\"\"\n",
        "        if self.mean is None:\n",
        "            return x\n",
        "        return x * self.std + self.mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNm5tYllZ-el"
      },
      "source": [
        "### Model Definition: Graph U-Net\n",
        "\n",
        "This cell generates `model.py`. We utilize the standard **GraphUNet** implementation from PyTorch Geometric.\n",
        "\n",
        "The `create_model` function acts as a factory:\n",
        "1.  It instantiates the U-Net architecture using the hyperparameters defined in `Config` (depth, hidden channels, pooling ratios).\n",
        "2.  It automatically moves the model to the appropriate computational device (GPU/CPU).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiP8xWE9aBYp",
        "outputId": "113bd60d-612b-4557-a5dc-268c70f44bdf"
      },
      "outputs": [],
      "source": [
        "%%writefile model.py\n",
        "from torch_geometric.nn import GraphUNet\n",
        "\n",
        "def create_model(config):\n",
        "    \"\"\"\n",
        "    Instantiates the Graph U-Net architecture.\n",
        "    \"\"\"\n",
        "    # We use the standard GraphUNet implementation from PyTorch Geometric.\n",
        "    # This architecture uses a U-shaped design with Graph Pooling (gPool)\n",
        "    # and Unpooling (gUnpool) operations to capture hierarchical features.\n",
        "    model = GraphUNet(\n",
        "        in_channels=config.IN_CHANNELS,         # Input features per node (12)\n",
        "        hidden_channels=config.HIDDEN_CHANNELS, # Width of the hidden layers\n",
        "        out_channels=config.OUT_CHANNELS,       # Output target (1 scalar: von Mises stress)\n",
        "        depth=config.DEPTH,                     # Number of pooling/unpooling steps\n",
        "        pool_ratios=config.POOL_RATIOS,         # Ratio of nodes kept after each pooling layer\n",
        "        sum_res=True,                           # Enables residual connections (Skip connections)\n",
        "        act='relu'                              # Activation function\n",
        "    )\n",
        "\n",
        "    # Move the entire model to the selected hardware (GPU/CPU)\n",
        "    model = model.to(config.DEVICE)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoPbwAUDaE7k"
      },
      "source": [
        "### Data Loading & Reconstruction\n",
        "\n",
        "This cell generates `data_loading.py`. Because our dataset is stored in a raw PyTorch Geometric format (a tuple of concatenated tensors and slice indices) rather than a list of objects, we need a custom loading pipeline.\n",
        "\n",
        "Key steps include:\n",
        "1.  **Module Patching:** We manually inject `UnitGaussianNormalizer` into `sys.modules` to prevent pickling errors during `torch.load`.\n",
        "2.  **Graph Reconstruction:** The `get_graph_from_tuple` function manually slices the large concatenated tensors back into individual `Data` objects using the stored indices.\n",
        "3.  **DataLoaders:** Finally, we organize the graphs into `DataLoader` instances to handle efficient mini-batching for the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjUIXF_aaGKO",
        "outputId": "99054af0-79d0-4270-c968-3c5953c255e9"
      },
      "outputs": [],
      "source": [
        "%%writefile data_loading.py\n",
        "import torch\n",
        "import sys\n",
        "import types\n",
        "from torch_geometric.data import Data, InMemoryDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from utils import UnitGaussianNormalizer\n",
        "\n",
        "# --- CRITICAL WORKAROUND ---\n",
        "# The dataset was saved (pickled) with a reference to 'utils.UnitGaussianNormalizer'.\n",
        "# During loading, PyTorch looks for this exact module path. Since we are running in a\n",
        "# notebook/script environment where 'utils' might be defined differently, we manually\n",
        "# inject the class into sys.modules to prevent an AttributeError during torch.load.\n",
        "sys.modules['utils'] = types.ModuleType('utils')\n",
        "sys.modules['utils'].UnitGaussianNormalizer = UnitGaussianNormalizer\n",
        "\n",
        "def get_graph_from_tuple(data, slices, idx):\n",
        "    \"\"\"\n",
        "    Reconstructs a single PyTorch Geometric Data object from the large concatenated\n",
        "    tensors using slice indices.\n",
        "\n",
        "    Args:\n",
        "        data: The huge object containing concatenated attributes (x, edge_index, etc.) for all graphs.\n",
        "        slices: A dictionary storing the start/end indices for each attribute.\n",
        "        idx: The index of the specific graph to retrieve.\n",
        "    \"\"\"\n",
        "    data_dict = {}\n",
        "    for key in slices.keys():\n",
        "        start, end = slices[key][idx].item(), slices[key][idx + 1].item()\n",
        "\n",
        "        # Handle 2D tensors (like edge_index or faces) differently than 1D attributes\n",
        "        if key in ['edge_index', 'face'] and data[key].dim() == 2:\n",
        "            data_dict[key] = data[key][:, start:end]\n",
        "        else:\n",
        "            data_dict[key] = data[key][start:end]\n",
        "\n",
        "    return Data(**data_dict)\n",
        "\n",
        "def load_data(config):\n",
        "    \"\"\"\n",
        "    Loads training and validation datasets from .pt files, applies subsetting,\n",
        "    and returns DataLoaders for the GNN.\n",
        "    \"\"\"\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    # Load Training Data\n",
        "    # weights_only=False is required here because the file contains complex objects\n",
        "    # (like the Normalizer class instance), not just state dictionaries.\n",
        "    train_dataset_tuple = torch.load(config.TRAIN_DATA_PATH, weights_only=False)\n",
        "    train_data, train_slices = train_dataset_tuple\n",
        "\n",
        "    # --- Subset Training Data (Optional) ---\n",
        "    # Reduces the dataset size based on TRAIN_SUBSET_RATIO (useful for faster debugging)\n",
        "    num_graphs = train_slices['x'].size(0) - 1\n",
        "    keep = int(num_graphs * config.TRAIN_SUBSET_RATIO)\n",
        "\n",
        "    new_slices = {}\n",
        "    for key in train_slices.keys():\n",
        "        new_slices[key] = train_slices[key][:keep+1].clone()\n",
        "    train_slices = new_slices\n",
        "\n",
        "    # Load Validation Data\n",
        "    test_dataset_tuple = torch.load(config.VAL_DATA_PATH, weights_only=False)\n",
        "    test_data, test_slices = test_dataset_tuple\n",
        "\n",
        "    # Reconstruct individual Data objects list from the massive storage tensors\n",
        "    # This might take a few seconds but makes iteration easier.\n",
        "    train_graphs = [get_graph_from_tuple(train_data, train_slices, i)\n",
        "                    for i in range(train_slices['x'].size(0) - 1)]\n",
        "    val_graphs = [get_graph_from_tuple(test_data, test_slices, i)\n",
        "                  for i in range(test_slices['x'].size(0) - 1)]\n",
        "\n",
        "    # Create PyTorch Geometric DataLoaders\n",
        "    # These handle the dynamic batching (diagonal stacking of adjacency matrices)\n",
        "    train_loader = DataLoader(train_graphs, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_graphs, batch_size=config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, len(train_graphs), len(val_graphs), train_graphs, val_graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21humdTIaISk"
      },
      "source": [
        "### Training Loop & Metric Tracking\n",
        "\n",
        "This block defines the core training and evaluation logic:\n",
        "\n",
        "1.  **`validate`**: This function goes beyond simple loss calculation. It computes the **$R^2$ score for each individual geometry** in the validation set. It then calculates the 10th, 50th, and 90th percentiles to measure model robustness and identifies the indices of the best, median, and worst performing graphs for later visualization.\n",
        "2.  **`train_model`**: The main loop that iterates through epochs, updates weights, and saves the model checkpoint (`best_model.pt`) whenever validation loss improves.\n",
        "3.  **`plot_metrics`**: A utility to generate comprehensive learning curves, allowing us to visualize convergence and the stability of predictions across different percentiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dknm0K4KaJmb",
        "outputId": "4f6f3f25-b84e-4938-ff8e-3160586b6eff"
      },
      "outputs": [],
      "source": [
        "%%writefile training.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, device, gradient_clip, num_train_graphs):\n",
        "    \"\"\"Performs one training epoch.\"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        loss = F.huber_loss(out.view(-1), batch.y.view(-1), delta=0.5)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * batch.num_graphs\n",
        "    return train_loss / num_train_graphs\n",
        "\n",
        "def validate(model, val_loader, device, num_val_graphs):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the validation set.\n",
        "    Calculates global metrics (RMSE, MAE) and per-graph R2 scores.\n",
        "    Returns the indices of the Best, Median, and Worst performing graphs.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    # Lists for global RMSE/MAE calculation\n",
        "    all_preds_global = []\n",
        "    all_targets_global = []\n",
        "\n",
        "    # List to store (global_index, r2_score) tuples\n",
        "    # Allows tracking which graph corresponds to which score\n",
        "    graph_scores_with_indices = []\n",
        "\n",
        "    # Offset to track the real global index across batches\n",
        "    global_idx_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "\n",
        "            # Loss Calculation\n",
        "            loss = F.huber_loss(out.view(-1), batch.y.view(-1), delta=0.5)\n",
        "            val_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "            # Data Prep (Move to CPU for sklearn metrics)\n",
        "            preds_cpu = out.view(-1).cpu().numpy()\n",
        "            targets_cpu = batch.y.view(-1).cpu().numpy()\n",
        "            batch_indices = batch.batch.cpu().numpy()\n",
        "\n",
        "            all_preds_global.append(preds_cpu)\n",
        "            all_targets_global.append(targets_cpu)\n",
        "\n",
        "            # Calculate R2 per individual graph\n",
        "            unique_graphs = np.unique(batch_indices)\n",
        "\n",
        "            for graph_id in unique_graphs:\n",
        "                mask = (batch_indices == graph_id)\n",
        "                graph_targets = targets_cpu[mask]\n",
        "                graph_preds = preds_cpu[mask]\n",
        "\n",
        "                # Compute R2 only if graph has more than 1 node\n",
        "                if len(graph_targets) > 1:\n",
        "                    score = r2_score(graph_targets, graph_preds)\n",
        "\n",
        "                    # Calculate absolute index in the validation dataset\n",
        "                    real_global_index = global_idx_offset + graph_id\n",
        "                    graph_scores_with_indices.append((real_global_index, score))\n",
        "\n",
        "            # Update offset for next batch\n",
        "            global_idx_offset += batch.num_graphs\n",
        "\n",
        "    val_loss /= num_val_graphs\n",
        "\n",
        "    metrics = {}\n",
        "    selected_indices = [None, None, None] # [Best, Mean, Worst]\n",
        "\n",
        "    if len(all_preds_global) > 0:\n",
        "        final_preds = np.concatenate(all_preds_global)\n",
        "        final_targets = np.concatenate(all_targets_global)\n",
        "\n",
        "        metrics['RMSE'] = np.sqrt(mean_squared_error(final_targets, final_preds))\n",
        "        metrics['MAE'] = mean_absolute_error(final_targets, final_preds)\n",
        "\n",
        "        if len(graph_scores_with_indices) > 0:\n",
        "            # Separate indices and scores\n",
        "            indices_arr = np.array([item[0] for item in graph_scores_with_indices])\n",
        "            scores_arr = np.array([item[1] for item in graph_scores_with_indices])\n",
        "\n",
        "            # 1. Calculate R2 percentiles (10th, 50th, 90th)\n",
        "            metrics['R2_90PCT'] = np.percentile(scores_arr, 90)\n",
        "            metrics['R2_50PCT'] = np.percentile(scores_arr, 50)\n",
        "            metrics['R2_10PCT'] = np.percentile(scores_arr, 10)\n",
        "\n",
        "            # 2. Identify key graph indices for visualization\n",
        "\n",
        "            # A. Best Case (Max R2)\n",
        "            idx_best = indices_arr[np.argmax(scores_arr)]\n",
        "\n",
        "            # B. Worst Case (Min R2)\n",
        "            idx_worst = indices_arr[np.argmin(scores_arr)]\n",
        "\n",
        "            # C. Median/Average Case (Closest to mean R2)\n",
        "            mean_score = np.mean(scores_arr)\n",
        "            idx_closest_to_mean = indices_arr[(np.abs(scores_arr - mean_score)).argmin()]\n",
        "\n",
        "            # Return order: [Best, Median, Worst]\n",
        "            selected_indices = [int(idx_best), int(idx_closest_to_mean), int(idx_worst)]\n",
        "\n",
        "        else:\n",
        "            metrics['R2_90PCT'] = 0; metrics['R2_50PCT'] = 0; metrics['R2_10PCT'] = 0\n",
        "\n",
        "    return val_loss, metrics, selected_indices\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, config, num_train_graphs, num_val_graphs):\n",
        "    \"\"\"\n",
        "    Main training loop.\n",
        "    Iterates over epochs, runs validation, saves the best model, and logs history.\n",
        "    \"\"\"\n",
        "    print(f\"Starting training on {config.DEVICE}...\")\n",
        "\n",
        "    # Initialize history dictionary\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'RMSE': [], 'MAE': [],\n",
        "        'R2_90PCT': [], 'R2_50PCT': [], 'R2_10PCT': []\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(1, config.NUM_EPOCHS + 1):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, config.DEVICE, config.GRADIENT_CLIP, num_train_graphs)\n",
        "        val_loss, metrics, index = validate(model, val_loader, config.DEVICE, num_val_graphs)\n",
        "\n",
        "        # Update History\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['RMSE'].append(metrics['RMSE'])\n",
        "        history['MAE'].append(metrics['MAE'])\n",
        "        history['R2_90PCT'].append(metrics['R2_90PCT'])\n",
        "        history['R2_50PCT'].append(metrics['R2_50PCT'])\n",
        "        history['R2_10PCT'].append(metrics['R2_10PCT'])\n",
        "\n",
        "        print(f\"Ep {epoch:03d} | Val: {val_loss:.4f} | RMSE: {metrics['RMSE']:.3f} | \"\n",
        "              f\"R2(10/50/90): {metrics['R2_10PCT']:.2f} / {metrics['R2_50PCT']:.2f} / {metrics['R2_90PCT']:.2f}\")\n",
        "\n",
        "        # Checkpoint: Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), config.BEST_MODEL_PATH)\n",
        "\n",
        "    return history, best_val_loss, index\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"\n",
        "    Generates and saves 3 plots:\n",
        "    1. Training vs Validation Loss\n",
        "    2. Global Error Metrics (RMSE, MAE)\n",
        "    3. R2 Score Distribution (10th, 50th, 90th percentiles)\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    fig, axs = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
        "\n",
        "    # 1. Loss Curves\n",
        "    axs[0].plot(epochs, history['train_loss'], label=\"Train Loss\", color='blue')\n",
        "    axs[0].plot(epochs, history['val_loss'], label=\"Val Loss\", color='red')\n",
        "    axs[0].set_ylabel(\"Huber Loss\")\n",
        "    axs[0].set_title(\"Training & Validation Loss\")\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # 2. Error Metrics\n",
        "    axs[1].plot(epochs, history['RMSE'], label=\"RMSE\", color='orange')\n",
        "    axs[1].plot(epochs, history['MAE'], label=\"MAE\", color='green')\n",
        "    axs[1].set_ylabel(\"Error Units\")\n",
        "    axs[1].set_title(\"Global Error Metrics\")\n",
        "    axs[1].legend()\n",
        "    axs[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # 3. R2 Percentiles\n",
        "    axs[2].plot(epochs, history['R2_90PCT'], label=\"R2 Best (90%)\", linestyle='--', color='purple')\n",
        "    axs[2].plot(epochs, history['R2_50PCT'], label=\"R2 Median (50%)\", linewidth=2, color='black')\n",
        "    axs[2].plot(epochs, history['R2_10PCT'], label=\"R2 Worst (10%)\", linestyle=':', color='brown')\n",
        "\n",
        "    # Add zero line to highlight negative scores\n",
        "    axs[2].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
        "\n",
        "    axs[2].set_ylabel(\"R2 Score\")\n",
        "    axs[2].set_xlabel(\"Epochs\")\n",
        "    axs[2].set_title(\"R2 Score Distribution (Per Geometry)\")\n",
        "    axs[2].legend(loc='lower right')\n",
        "    axs[2].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"Learning curves saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y89BfTtaL59"
      },
      "source": [
        "### Main Execution Pipeline\n",
        "\n",
        "This cell generates `main.py`, which serves as the orchestrator for our experiment. It integrates the modular components defined previously (Config, Data, Model, Training).\n",
        "\n",
        "The `main()` function performs the following steps:\n",
        "1.  **Initialization:** Instantiates the configuration and loads the dataset.\n",
        "2.  **Setup:** Builds the model and initializes the **Adam optimizer**.\n",
        "3.  **Execution:** Runs the training loop and generates the learning curves.\n",
        "4.  **Returns:** Crucially, it returns the trained `model`, the `val_graph` dataset, and the `index` list (containing IDs of the best/worst predictions). These are essential for the **qualitative analysis** in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpkrFPstaMlj",
        "outputId": "1c08dd54-91e1-4b03-a7b5-876bcec9c570"
      },
      "outputs": [],
      "source": [
        "%%writefile main.py\n",
        "import torch\n",
        "from config import Config\n",
        "from data_loading import load_data\n",
        "from model import create_model\n",
        "from training import train_model, plot_metrics\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Orchestrates the entire machine learning pipeline:\n",
        "    1. Configuration setup\n",
        "    2. Data loading\n",
        "    3. Model initialization\n",
        "    4. Training loop execution\n",
        "    5. Result visualization\n",
        "    \"\"\"\n",
        "    # 1. Initialize configuration parameters\n",
        "    config = Config()\n",
        "\n",
        "    # 2. Load datasets and create DataLoaders\n",
        "    # Returns loaders for batching and raw lists of graphs for analysis\n",
        "    train_loader, val_loader, num_train, num_val, train_graph, val_graph = load_data(config)\n",
        "\n",
        "    # 3. Initialize the Graph Neural Network model\n",
        "    model = create_model(config)\n",
        "\n",
        "    # 4. Define the Optimizer (Adam)\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config.LEARNING_RATE,\n",
        "        weight_decay=config.WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    # 5. Execute the training loop\n",
        "    # Returns:\n",
        "    # - history: Dictionary containing loss and metrics over epochs\n",
        "    # - best_val_loss: The lowest validation loss achieved\n",
        "    # - index: List of indices for [Best, Median, Worst] validation cases\n",
        "    history, best_val_loss, index = train_model(\n",
        "        model, train_loader, val_loader, optimizer, config, num_train, num_val\n",
        "    )\n",
        "\n",
        "    # 6. Generate and save training curves (Loss, RMSE, R2)\n",
        "    plot_metrics(history, config.TRAINING_CURVE_PATH)\n",
        "\n",
        "    # Return key objects for the subsequent visualization steps in the notebook\n",
        "    return model, index, val_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhtQwbo-crQZ"
      },
      "source": [
        "### Execution\n",
        "\n",
        "This cell runs the entire pipeline defined in `main.py`. It trains the model, plots the learning curves, and returns the best/worst case indices needed for the qualitative analysis in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xW8wRPuEq6FX",
        "outputId": "4eecb39e-efc5-42a9-8914-7209d5c091cb"
      },
      "outputs": [],
      "source": [
        "from main import main\n",
        "\n",
        "\n",
        "model, index, val_graph = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwUNGdjlpuVS"
      },
      "source": [
        "### Qualitative Analysis: 3D Stress Field Visualization\n",
        "\n",
        "This cell performs the final visual inspection of the model's predictions. We iterate through the specific test cases identified earlier: **Best, Median, and Worst** performance.\n",
        "\n",
        "**Key Feature: Independent Color Scaling**\n",
        "The `visualize_side_by_side_independent` function allows the color map to scale dynamically for each plot:\n",
        "* **Left (Target):** Ground truth from FEM.\n",
        "* **Right (Prediction):** GNN output.\n",
        "\n",
        "**Why independent scales?**\n",
        "This allows us to verify if the model has learned the correct **stress distribution patterns** (topology of the hotspots), even if the absolute **magnitudes** are underestimated (a common issue known as \"over-smoothing\" in regression tasks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ezeHsb9HptyE",
        "outputId": "b6f77ebe-ad85-4d91-b8a4-2c9e827fab7d"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import torch\n",
        "from config import Config\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# Import the main module and reload it to ensure latest changes are used\n",
        "import main\n",
        "importlib.reload(main)\n",
        "\n",
        "# --- 1. SIDE-BY-SIDE VISUALIZATION FUNCTION (AUTO-SCALE) ---\n",
        "def visualize_side_by_side_independent(target_data, pred_data, index):\n",
        "    \"\"\"\n",
        "    Displays the ground truth and the prediction side by side.\n",
        "    Each plot uses its OWN color scale to visualize patterns effectively,\n",
        "    independently of the value magnitude.\n",
        "    \"\"\"\n",
        "\n",
        "    # Face verification\n",
        "    if not hasattr(target_data, 'face') or target_data.face is None:\n",
        "        print(\"⚠️ No faces detected.\")\n",
        "        return\n",
        "\n",
        "    # --- Data Extraction ---\n",
        "    x = target_data.x[:, 0].cpu().numpy()\n",
        "    y = target_data.x[:, 1].cpu().numpy()\n",
        "    z = target_data.x[:, 2].cpu().numpy()\n",
        "    faces = target_data.face.cpu().numpy()\n",
        "    i, j, k = faces[0], faces[1], faces[2]\n",
        "\n",
        "    val_target = target_data.y[:, 0].cpu().numpy()\n",
        "    val_pred = pred_data.y[:, 0].cpu().numpy()\n",
        "\n",
        "    # --- Figure Creation ---\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=2,\n",
        "        specs=[[{'type': 'scene'}, {'type': 'scene'}]],\n",
        "        subplot_titles=(\n",
        "            f\"TARGET (Max: {val_target.max():.2f})\",\n",
        "            f\"PREDICTION (Max: {val_pred.max():.2f})\"\n",
        "        ),\n",
        "        horizontal_spacing=0.05\n",
        "    )\n",
        "\n",
        "    # --- Trace 1: Left (Target) ---\n",
        "    fig.add_trace(\n",
        "        go.Mesh3d(\n",
        "            x=x, y=y, z=z, i=i, j=j, k=k,\n",
        "            intensity=val_target,\n",
        "            intensitymode='vertex',\n",
        "            colorscale='Jet',\n",
        "            # HERE: Force scale based on TARGET data only\n",
        "            cmin=val_target.min(),\n",
        "            cmax=val_target.max(),\n",
        "            opacity=1.0,\n",
        "            name='Target',\n",
        "            colorbar=dict(title=\"Target\", x=0.45, len=0.8)\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # --- Trace 2: Right (Prediction) ---\n",
        "    fig.add_trace(\n",
        "        go.Mesh3d(\n",
        "            x=x, y=y, z=z, i=i, j=j, k=k,\n",
        "            intensity=val_pred,\n",
        "            intensitymode='vertex',\n",
        "            colorscale='Jet',\n",
        "            # HERE: Force scale based on PREDICTION data only\n",
        "            cmin=val_pred.min(),\n",
        "            cmax=val_pred.max(),\n",
        "            opacity=1.0,\n",
        "            name='Prediction',\n",
        "            colorbar=dict(title=\"Pred\", x=1.0, len=0.8) # A second legend bar on the right\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # --- Layout ---\n",
        "    fig.update_layout(\n",
        "        title_text=f\"Stress Pattern Comparison (Part {index}) - Independent Scales\",\n",
        "        height=600, width=1200,\n",
        "        margin=dict(r=0, b=0, l=0, t=50),\n",
        "        scene=dict(aspectmode='data'),\n",
        "        scene2=dict(aspectmode='data')\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "# --- 2. EXECUTION ---\n",
        "\n",
        "for i in index:\n",
        "  print(f\"--- Comparison (Decoupled Scales) for Part #{i} ---\")\n",
        "\n",
        "  # Retrieval and calculation\n",
        "  target_graph = val_graph[i].clone().to(Config.DEVICE)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      pred_tensor = model(target_graph.x, target_graph.edge_index, batch=None)\n",
        "\n",
        "  prediction_graph = target_graph.clone()\n",
        "  prediction_graph.y = pred_tensor\n",
        "\n",
        "  # Display (Function call matches the definition above)\n",
        "  visualize_side_by_side_independent(target_graph, prediction_graph, i)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Pytorch_Geo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kahLzPgQ6osF"
   },
   "source": [
    "## ClofNet Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYnpUzzZ6xRu"
   },
   "source": [
    "###  Drive Mount & Installation\n",
    "\n",
    "We mount Google Drive to access the data and install the required **PyTorch Geometric** libraries for our ClofNet GNN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33063,
     "status": "ok",
     "timestamp": 1765992239294,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "WDjfmXoN2sAQ",
    "outputId": "827ed0f9-d506-458c-8fa0-e069573ebce3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "\n",
    "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.0+cu121.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15127,
     "status": "ok",
     "timestamp": 1765992254424,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "JAhmq6xX746U",
    "outputId": "e95dec58-470a-4d93-aee5-786c6e5860ff"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import types\n",
    "import os\n",
    "import importlib\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torch import nn\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from plotly.subplots import make_subplots\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMe1aKmO6b1Q"
   },
   "source": [
    "###  Configuration & Hyperparameters\n",
    "\n",
    "This cell generates the `config.py` file, which centralizes all experiment parameters. This allows for quick adjustments to the configuration without modifying the model or training code directly.\n",
    "\n",
    "Key definitions include:\n",
    "* **Data Paths:** Absolute paths to the dataset (mounted via Google Drive).\n",
    "* **GNN Architecture:** Hidden channel dimensions (`HIDDEN_CHANNELS`), depth (`DEPTH`)...\n",
    "* **Optimization:** Batch size, learning rate, and number of epochs.\n",
    "* **Hardware:** Automatic selection of GPU (CUDA) if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765992254435,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "Eiv10__n4ZYo"
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION Parameters\n",
    "class Config:\n",
    "\n",
    "    TRAIN_DATA_PATH = '/content/gdrive/MyDrive/ml_lab_project/Data_lab/train_data.pt'\n",
    "    VAL_DATA_PATH = '/content/gdrive/MyDrive/ml_lab_project/Data_lab/val_data.pt'\n",
    "\n",
    "    # Data Parameters\n",
    "    TRAIN_SUBSET_RATIO = 1.0   # 100% of dataset\n",
    "    BATCH_SIZE = 64            # Number of graphs processed in parallel\n",
    "\n",
    "    # Model Architecture (ClofNet)\n",
    "    IN_NODE_FEATURES = 9       # Input features from dataset\n",
    "    HIDDEN_CHANNELS = 64       # Hidden dimension size\n",
    "    OUT_STRESS_DIM = 1         # Scalar output (Von Mises stress)\n",
    "    DEPTH = 4                  # Number of ClofNet layers\n",
    "\n",
    "    # ClofNet Specifics\n",
    "    NORM_DIFF = True           # Normalize difference vectors\n",
    "    TANH = False               # No limit on virtual movement\n",
    "    COORDS_WEIGHT = 1.0        # Let the network move the nodes\n",
    "\n",
    "    # Training Hyperparameters\n",
    "    NUM_EPOCHS = 50             # Total training iterations\n",
    "    LEARNING_RATE = 0.001      # Step size for the optimizer\n",
    "    WEIGHT_DECAY = 1e-12       # L2 Regularization to prevent overfitting\n",
    "    GRADIENT_CLIP = 1.0        # Max norm for gradients (stabilizes training)\n",
    "\n",
    "    # --- Saving ---\n",
    "    BEST_MODEL_PATH = 'best_model.pt'\n",
    "    TRAINING_CURVE_PATH = 'training_curve_clofnet.png'\n",
    "\n",
    "    # --- Hardware ---\n",
    "    # Automatically select GPU if available for faster computation\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNGD08jMB0wd"
   },
   "source": [
    "###  Data Loading Compatibility\n",
    "\n",
    "Although the dataset is already normalized, the saved `.pt` files contain instances of `UnitGaussianNormalizer`. We explicitly define this class here to ensure `torch.load` can deserialize the data correctly without raising `AttributeError` or warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1765992254442,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "VKqgRZOeA2G4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This class is necessary to load objects saved with 'utils' in the checkpoint\n",
    "class UnitGaussianNormalizer:\n",
    "    def __init__(self, x=None, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.mean = x.mean(dim=0, keepdim=True) if x is not None else None\n",
    "        self.std = x.std(dim=0, keepdim=True) + eps if x is not None else None\n",
    "    def encode(self, x): return (x - self.mean) / self.std if self.mean is not None else x\n",
    "    def decode(self, x, sample_idx=None): return x * self.std + self.mean if self.mean is not None else x\n",
    "\n",
    "#sys.modules['utils'] = types.ModuleType('utils')\n",
    "#sys.modules['utils'].UnitGaussianNormalizer = UnitGaussianNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5wqVXiu7y00"
   },
   "source": [
    "\n",
    "### ClofNet Architecture Definition\n",
    "\n",
    "This cell implements the **ClofNet (Complete Local Frames Network)** architecture from the github (https://github.com/mouthful/ClofNet), adapted here for static stress prediction.  \n",
    "ClofNet explicitly models 3D geometry by constructing a unique, rotation-equivariant coordinate system (local frame) for every edge.\n",
    "\n",
    "Key components include:\n",
    "\n",
    "1.  **Scatter Helpers:** `unsorted_segment_sum` and `unsorted_segment_mean` provide efficient, vectorized aggregation of messages from neighbors, which is essential for the message-passing paradigm.\n",
    "2.  **`Clof_GCL` (Graph Convolutional Layer):** The core building block.\n",
    "    * **`coord2localframe`:** Constructs an orthonormal basis for each edge using cross-products of node positions.\n",
    "    * **`scalarization`:** Projects absolute node coordinates onto these local frames to generate **9 rotation-invariant geometric features** (radial distance + 8 projections/angles). This allows the network to \"see\" anisotropy without breaking physical symmetries.\n",
    "    * **`coord_model`:** Updates node positions by predicting scalar coefficients $(\\alpha, \\beta, \\gamma)$ that weight the movement along the local frame axes, ensuring SE(3) equivariance.\n",
    "3.  **`ClofNetModel` (Wrapper):**\n",
    "    * Embeds the initial physical features (dimension 9) into a hidden latent space (dimension 64).\n",
    "    * Stacks multiple `Clof_GCL` layers (defined by `config.DEPTH`).\n",
    "    * **Safety Mechanism:** In the `forward` pass, we explicitly use `pos = pos.clone()` to ensure the coordinate updates operate on a temporary copy, preventing the corruption of the original dataset geometry in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1765992254484,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "oHwc-6c-71dL"
   },
   "outputs": [],
   "source": [
    "#--- Helper functions for scatter operations ---\n",
    "# These functions implement the \"Message Aggregation\" step of Graph Neural Networks.\n",
    "# They take messages from edges and sum/mean them into the target nodes.\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    \"\"\"\n",
    "    Computes the mean of 'data' for each segment (node) specified by 'segment_ids'.\n",
    "    Used to average coordinate updates or features from neighbors.\n",
    "    \"\"\"\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)\n",
    "    count = data.new_full(result_shape, 0)\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    count.scatter_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)\n",
    "\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    \"\"\"\n",
    "    Computes the sum of 'data' for each segment (node).\n",
    "    Standard aggregation for message passing (e.g., summing edge features into nodes).\n",
    "    \"\"\"\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "# --- Core ClofNet Layer ---\n",
    "class Clof_GCL(nn.Module):\n",
    "    \"\"\"\n",
    "    ClofNet Graph Convolutional Layer.\n",
    "\n",
    "    This layer implements SE(3) equivariance by constructing a 'Complete Local Frame'\n",
    "    for every edge. Instead of learning on absolute coordinates (which change with rotation),\n",
    "    it projects geometry onto these local frames to learn invariant features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nf, output_nf, hidden_nf, edges_in_d=0,\n",
    "                 act_fn=nn.SiLU(), recurrent=True, coords_weight=1.0,\n",
    "                 norm_diff=True, tanh=False):\n",
    "        super(Clof_GCL, self).__init__()\n",
    "        self.coords_weight = coords_weight\n",
    "        self.recurrent = recurrent\n",
    "        self.norm_diff = norm_diff\n",
    "        self.tanh = tanh\n",
    "\n",
    "        # MLP to process edge features + geometric invariants\n",
    "        input_edge = input_nf * 2\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(input_edge + edges_in_d, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, hidden_nf),\n",
    "            act_fn\n",
    "        )\n",
    "\n",
    "        # MLP to update node features\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_nf + input_nf, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, output_nf)\n",
    "        )\n",
    "\n",
    "        # MLP to predict scalar coefficients for coordinate updates\n",
    "        layer = nn.Linear(hidden_nf, 3, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "        self.coord_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_nf, hidden_nf),\n",
    "            act_fn,\n",
    "            layer\n",
    "        )\n",
    "        if self.tanh:\n",
    "            self.coord_mlp.append(nn.Tanh())\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_nf)\n",
    "\n",
    "    def coord2localframe(self, edge_index, coord):\n",
    "        \"\"\"\n",
    "        Constructs the Local Frame (Orthonormal Basis) for each edge.\n",
    "\n",
    "        Basis vectors:\n",
    "        1. Radial (a_ij): Direction along the edge (normalized).\n",
    "        2. Cross (b_ij): Orthogonal to Radial, derived from position cross-product.\n",
    "        3. Vertical (c_ij): Completes the basis (a_ij x b_ij).\n",
    "        \"\"\"\n",
    "        row, col = edge_index\n",
    "        coord_diff = coord[row] - coord[col]\n",
    "        radial = torch.sum((coord_diff)**2, 1).unsqueeze(1)\n",
    "        # Using linalg.cross to avoid warnings\n",
    "        coord_cross = torch.linalg.cross(coord[row], coord[col], dim=-1)\n",
    "\n",
    "        if self.norm_diff:\n",
    "            norm = torch.sqrt(radial) + 1e-8\n",
    "            coord_diff = coord_diff / norm\n",
    "            cross_norm = torch.sqrt(torch.sum((coord_cross)**2, 1).unsqueeze(1)) + 1e-8\n",
    "            coord_cross = coord_cross / cross_norm\n",
    "\n",
    "        coord_vertical = torch.cross(coord_diff, coord_cross, dim=-1)\n",
    "        return radial, coord_diff, coord_cross, coord_vertical\n",
    "\n",
    "    def edge_model(self, source, target, radial, edge_attr, coff_feat):\n",
    "        \"\"\"Merges node features, radial distance, and geometric invariants (coff_feat).\"\"\"\n",
    "        if edge_attr is None:\n",
    "            out = torch.cat([source, target, radial, coff_feat], dim=1)\n",
    "        else:\n",
    "            out = torch.cat([source, target, radial, edge_attr, coff_feat], dim=1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "    def node_model(self, x, edge_index, edge_attr):\n",
    "        \"\"\"Aggregates edge messages to update node features (Standard MPNN).\"\"\"\n",
    "        row, col = edge_index\n",
    "        agg = unsorted_segment_sum(edge_attr, row, num_segments=x.size(0))\n",
    "        out = torch.cat([x, agg], dim=1)\n",
    "        out = self.node_mlp(out)\n",
    "        if self.recurrent:\n",
    "            out = x + out\n",
    "        return out\n",
    "\n",
    "    def coord_model(self, coord, edge_index, coord_diff, coord_cross, coord_vertical, edge_feat):\n",
    "        \"\"\"\n",
    "        Updates node coordinates EQUIVARIANTLY.\n",
    "\n",
    "        Instead of predicting a raw displacement vector (dx, dy, dz), it predicts\n",
    "        3 scalar coefficients (alpha, beta, gamma). The displacement is constructed\n",
    "        by weighting the Local Frame axes:\n",
    "        Delta_x = alpha * Radial + beta * Cross + gamma * Vertical\n",
    "        \"\"\"\n",
    "        row, col = edge_index\n",
    "        coff = self.coord_mlp(edge_feat)\n",
    "        trans = coord_diff * coff[:, :1] + coord_cross * coff[:, 1:2] + coord_vertical * coff[:, 2:3]\n",
    "        agg = unsorted_segment_mean(trans, row, num_segments=coord.size(0))\n",
    "        coord += agg * self.coords_weight\n",
    "        return coord\n",
    "\n",
    "    def scalarization(self, edge_index, coord):\n",
    "        \"\"\"\n",
    "        Projects absolute coordinates onto the Local Frame to create Invariant Features.\n",
    "\n",
    "        Outputs 9 Geometric Features:\n",
    "        - 1: Radial distance\n",
    "        - 3: Projections of node i onto frame (local coords of i)\n",
    "        - 3: Projections of node j onto frame (local coords of j)\n",
    "        - 2: Sin/Cos of relative angle\n",
    "        \"\"\"\n",
    "        radial, coord_diff, coord_cross, coord_vertical = self.coord2localframe(edge_index, coord)\n",
    "        row, col = edge_index\n",
    "        edge_basis = torch.stack([coord_diff, coord_cross, coord_vertical], dim=1)\n",
    "        r_i = coord[row].unsqueeze(-1)\n",
    "        r_j = coord[col].unsqueeze(-1)\n",
    "        coff_i = torch.matmul(edge_basis.transpose(1, 2), r_i).squeeze(-1)\n",
    "        coff_j = torch.matmul(edge_basis.transpose(1, 2), r_j).squeeze(-1)\n",
    "        coff_mul = coff_i * coff_j\n",
    "        coff_i_norm = coff_i.norm(dim=-1, keepdim=True) + 1e-5\n",
    "        coff_j_norm = coff_j.norm(dim=-1, keepdim=True) + 1e-5\n",
    "        pseudo_cos = coff_mul.sum(dim=-1, keepdim=True) / (coff_i_norm * coff_j_norm)\n",
    "        pseudo_sin = torch.sqrt(torch.clamp(1 - pseudo_cos**2, min=1e-5))\n",
    "        pseudo_angle = torch.cat([pseudo_sin, pseudo_cos], dim=-1)\n",
    "        coff_feat = torch.cat([pseudo_angle, coff_i, coff_j], dim=-1)\n",
    "        return radial, coord_diff, coord_cross, coord_vertical, coff_feat\n",
    "\n",
    "    def forward(self, h, edge_index, coord, edge_attr=None):\n",
    "        row, col = edge_index\n",
    "        residue = h\n",
    "        # 1. Compute Geometric Invariants (Scalarization)\n",
    "        radial, coord_diff, coord_cross, coord_vertical, coff_feat = self.scalarization(edge_index, coord)\n",
    "\n",
    "        # 2. Update Edge Features (Mixing Physics 'h' + Geometry 'coff_feat')\n",
    "        edge_feat = self.edge_model(h[row], h[col], radial, edge_attr, coff_feat)\n",
    "\n",
    "        # 3. Update Coordinates (Equivariant update)\n",
    "        coord = self.coord_model(coord, edge_index, coord_diff, coord_cross, coord_vertical, edge_feat)\n",
    "\n",
    "        # 4. Update Node Features\n",
    "        h = self.node_model(h, edge_index, edge_feat)\n",
    "        h = residue + h\n",
    "        h = self.layer_norm(h)\n",
    "        return h, coord\n",
    "\n",
    "# --- Main Model Wrapper ---\n",
    "class ClofNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full ClofNet Architecture.\n",
    "    Stacks multiple Clof_GCL layers and adds a final projection head for stress prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_nf = config.HIDDEN_CHANNELS\n",
    "        self.n_layers = config.DEPTH\n",
    "        # Embedding: Projects raw physical features (9 dim) -> Latent Space (64 dim)\n",
    "        self.embedding_node = nn.Linear(config.IN_NODE_FEATURES, self.hidden_nf)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            self.layers.append(\n",
    "                Clof_GCL(\n",
    "                    input_nf=self.hidden_nf,\n",
    "                    output_nf=self.hidden_nf,\n",
    "                    hidden_nf=self.hidden_nf,\n",
    "                    edges_in_d=9, # The 9 Geometric Features from Scalarization\n",
    "                    act_fn=nn.SiLU(),\n",
    "                    recurrent=True,\n",
    "                    coords_weight=config.COORDS_WEIGHT,\n",
    "                    norm_diff=config.NORM_DIFF,\n",
    "                    tanh=config.TANH\n",
    "                )\n",
    "            )\n",
    "        # Decoder: Latent Space -> Von Mises Stress (1 dim)\n",
    "        self.stress_head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_nf, self.hidden_nf),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.hidden_nf, config.OUT_STRESS_DIM)\n",
    "        )\n",
    "    def forward(self, h, pos, edge_index, batch=None):\n",
    "        # Clone pos to avoid modifying node coordinates firm the original dataset in memory\n",
    "        pos = pos.clone()\n",
    "\n",
    "        h = self.embedding_node(h)\n",
    "        for layer in self.layers:\n",
    "            h, pos = layer(h, edge_index, pos)\n",
    "\n",
    "        pred_stress = self.stress_head(h)\n",
    "        return pred_stress\n",
    "\n",
    "def create_model(config):\n",
    "    return ClofNetModel(config).to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PI7Yczi8AyJ"
   },
   "source": [
    "### Data Loading & Reconstruction\n",
    "\n",
    "This cell generates `data_loading.py`. Because our dataset is stored in a highly optimized \"monolithic\" format (concatenated tensors with slice indices) rather than a list of individual objects, we implement a custom reconstruction pipeline.\n",
    "\n",
    "Key steps include:\n",
    "1.  **Module Patching:** We perform a \"monkey patch\" by injecting `UnitGaussianNormalizer` into `sys.modules['utils']` to prevent `ModuleNotFoundError` during `torch.load` (resolving a legacy serialization issue).\n",
    "2.  **Index Correction (`safe_correct_indices`):** We define a specialized helper function to sanitize topology data. It detects if edge or face indices are \"global\" (pointing to absolute positions like 100-105 in the monolithic tensor) and shifts them back to local zero-based indices (0-5) for the extracted graph.\n",
    "3.  **Graph Reconstruction:** The `get_graph_from_tuple` function slices the large tensors into individual `Data` objects, separating the geometry (`pos`) from physical features (`x`) and applying the index correction.\n",
    "4.  **DataLoaders:** Finally, we wrap the reconstructed graphs in PyG `DataLoader` instances to handle efficient mini-batching and shuffling for the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1765992254499,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "jUf8MxEE8B8g"
   },
   "outputs": [],
   "source": [
    "# --- CRITICAL WORKAROUND ---\n",
    "# The dataset was saved (pickled) with a reference to 'utils.UnitGaussianNormalizer'.\n",
    "# During loading, PyTorch looks for this exact module path. Since we are running in a\n",
    "# notebook/script environment where 'utils' might be defined differently, we manually\n",
    "# inject the class into sys.modules to prevent an AttributeError during torch.load.\n",
    "\n",
    "sys.modules['utils'] = types.ModuleType('utils')\n",
    "sys.modules['utils'].UnitGaussianNormalizer = UnitGaussianNormalizer\n",
    "\n",
    "\n",
    "def safe_correct_indices(tensor, offset, num_nodes):\n",
    "    \"\"\"\n",
    "    Corrects global indices (from a large concatenated dataset) to local indices (0 to N).\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The edge_index or face tensor.\n",
    "        offset (int): The starting node index for this specific graph in the large dataset.\n",
    "        num_nodes (int): Number of nodes in this specific graph.\n",
    "    \"\"\"\n",
    "    if tensor.numel() == 0:\n",
    "        return tensor\n",
    "\n",
    "    min_idx, max_idx = tensor.min().item(), tensor.max().item()\n",
    "\n",
    "    # Case A: Indices are global. Subtract offset.\n",
    "    if min_idx >= offset:\n",
    "        return tensor - offset\n",
    "    # Case B: Indices are already local. Do nothing.\n",
    "    elif max_idx < num_nodes:\n",
    "        return tensor\n",
    "    # Case C: Fallback.\n",
    "    else:\n",
    "        return tensor - min_idx\n",
    "\n",
    "def get_graph_from_tuple(data, slices, idx):\n",
    "    \"\"\"\n",
    "    Reconstructs a single PyTorch Geometric Data object from the large concatenated\n",
    "    tensors using slice indices.\n",
    "    \"\"\"\n",
    "    # 1. Determine graph boundaries\n",
    "    node_start = slices['x'][idx].item()\n",
    "    node_end = slices['x'][idx + 1].item()\n",
    "    num_nodes = node_end - node_start\n",
    "\n",
    "    data_dict = {}\n",
    "    for key in slices.keys():\n",
    "        start, end = slices[key][idx].item(), slices[key][idx + 1].item()\n",
    "        item = data[key]\n",
    "\n",
    "        # 2. Extract AND Correct Indices for Topology (Edges/Faces)\n",
    "        if key in ['edge_index', 'face'] and isinstance(item, torch.Tensor) and item.dim() == 2:\n",
    "            extracted = item[:, start:end]\n",
    "            # Must correct indices to be local to this graph (0 to num_nodes)\n",
    "            data_dict[key] = safe_correct_indices(extracted, node_start, num_nodes)\n",
    "        else:\n",
    "            data_dict[key] = item[start:end]\n",
    "\n",
    "    # 3. Separate Geometry (pos) from Physics Features (x)\n",
    "    if isinstance(data_dict['x'], torch.Tensor):\n",
    "        full_x = data_dict['x']\n",
    "        # First 3 columns are Coordinates (x,y,z)\n",
    "        data_dict['pos'] = full_x[:, :3]\n",
    "        # Remaining 9 columns are Physical Features\n",
    "        data_dict['x'] = full_x[:, 3:]\n",
    "\n",
    "    return Data(**data_dict)\n",
    "\n",
    "def load_data(config):\n",
    "    \"\"\"\n",
    "    Main function to load the .pt files, reconstruct graph objects, and create DataLoaders.\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    train_tuple = torch.load(config.TRAIN_DATA_PATH, weights_only=False)\n",
    "    train_data, train_slices = train_tuple\n",
    "\n",
    "    test_tuple = torch.load(config.VAL_DATA_PATH, weights_only=False)\n",
    "    test_data, test_slices = test_tuple\n",
    "\n",
    "    # --- Subset Selection ---\n",
    "    num_train = train_slices['x'].size(0) - 1\n",
    "    keep = int(num_train * config.TRAIN_SUBSET_RATIO)\n",
    "\n",
    "    new_train_slices = {k: train_slices[k][:keep+1].clone() for k in train_slices.keys()}\n",
    "\n",
    "    print(f\"Reconstructing {keep} training graphs from monolithic data...\")\n",
    "    train_graphs = [get_graph_from_tuple(train_data, new_train_slices, i) for i in range(keep)]\n",
    "    val_graphs = [get_graph_from_tuple(test_data, test_slices, i) for i in range(test_slices['x'].size(0) - 1)]\n",
    "\n",
    "    # Create PyG DataLoaders\n",
    "    train_loader = DataLoader(train_graphs, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_graphs, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"Data ready. Train size: {len(train_graphs)}, Val size: {len(val_graphs)}\")\n",
    "    return train_loader, val_loader, train_graphs, val_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RRILeZs8ITN"
   },
   "source": [
    "### Training Loop & Metric Tracking\n",
    "\n",
    "This block defines the core training and evaluation logic:\n",
    "\n",
    "1.  **`validate`**: This function goes beyond simple loss calculation. It computes the **$R^2$ score for each individual geometry** in the validation set. It then calculates the 10th, 50th, and 90th percentiles to measure model robustness and identifies the indices of the best, median, and worst performing graphs for later visualization.\n",
    "2.  **`train_model`**: The main loop that iterates through epochs, updates weights, and saves the model checkpoint (`best_model.pt`) whenever validation loss improves.\n",
    "3.  **`plot_metrics`**: A utility to generate comprehensive learning curves, allowing us to visualize convergence and the stability of predictions across different percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1765992254531,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "IHcAMsNp8LBt"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device, gradient_clip):\n",
    "    \"\"\"Performs one training epoch.\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_graphs = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_stress = model(batch.x, batch.pos, batch.edge_index, batch.batch)\n",
    "        loss = F.huber_loss(pred_stress.view(-1), batch.y.view(-1), delta=0.5)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch.num_graphs\n",
    "        total_graphs += batch.num_graphs\n",
    "    return train_loss / total_graphs if total_graphs > 0 else 0\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set.\n",
    "    Calculates global metrics (RMSE, MAE) and per-graph R2 scores.\n",
    "    Returns the indices of the Best, Median, and Worst performing graphs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total_graphs = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_r2_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred_stress = model(batch.x, batch.pos, batch.edge_index, batch.batch)\n",
    "            loss = F.huber_loss(pred_stress.view(-1), batch.y.view(-1), delta=0.5)\n",
    "            val_loss += loss.item() * batch.num_graphs\n",
    "            total_graphs += batch.num_graphs\n",
    "\n",
    "            preds_np = pred_stress.view(-1).cpu().numpy()\n",
    "            targets_np = batch.y.view(-1).cpu().numpy()\n",
    "            batch_indices = batch.batch.cpu().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "            all_targets.append(targets_np)\n",
    "\n",
    "            unique_graphs = np.unique(batch_indices)\n",
    "            for graph_id in unique_graphs:\n",
    "                mask = (batch_indices == graph_id)\n",
    "                g_target = targets_np[mask]\n",
    "                g_pred = preds_np[mask]\n",
    "                if len(g_target) > 1:\n",
    "                    all_r2_scores.append(r2_score(g_target, g_pred))\n",
    "\n",
    "    val_loss /= total_graphs\n",
    "    final_preds = np.concatenate(all_preds)\n",
    "    final_targets = np.concatenate(all_targets)\n",
    "\n",
    "    # Calculate percentiles\n",
    "    r2_90, r2_50, r2_10 = 0, 0, 0\n",
    "    if len(all_r2_scores) > 0:\n",
    "        arr = np.array(all_r2_scores)\n",
    "        r2_90 = np.percentile(arr, 90)\n",
    "        r2_50 = np.percentile(arr, 50)\n",
    "        r2_10 = np.percentile(arr, 10)\n",
    "\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(final_targets, final_preds)),\n",
    "        'MAE': mean_absolute_error(final_targets, final_preds),\n",
    "        'all_r2': all_r2_scores,\n",
    "        'R2_90PCT': r2_90, 'R2_50PCT': r2_50, 'R2_10PCT': r2_10\n",
    "    }\n",
    "    return val_loss, metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, config):\n",
    "    \"\"\"\n",
    "    Main training loop.\n",
    "    Iterates over epochs, runs validation, saves the best model, and logs history.\n",
    "    \"\"\"\n",
    "    print(\"Starting ClofNet Training...\")\n",
    "    history = {'train_loss': [], 'val_loss': [], 'RMSE': [], 'MAE': [], 'R2_90PCT': [], 'R2_50PCT': [], 'R2_10PCT': []}\n",
    "    best_val_loss = float('inf')\n",
    "    last_metrics = {}\n",
    "\n",
    "    for epoch in range(1, config.NUM_EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.DEVICE, config.GRADIENT_CLIP)\n",
    "        val_loss, metrics = validate(model, val_loader, config.DEVICE)\n",
    "        last_metrics = metrics\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        for k in ['RMSE', 'MAE', 'R2_90PCT', 'R2_50PCT', 'R2_10PCT']: history[k].append(metrics.get(k, 0))\n",
    "\n",
    "        # EXACT FORMAT REQUIRED\n",
    "        print(f\"Ep {epoch:02d} | Val: {val_loss:.4f} | RMSE: {metrics['RMSE']:.3f} | \"\n",
    "              f\"R2(10/50/90): {metrics['R2_10PCT']:.2f} / {metrics['R2_50PCT']:.2f} / {metrics['R2_90PCT']:.2f} | \"\n",
    "              f\"Time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), config.BEST_MODEL_PATH)\n",
    "\n",
    "    return history, best_val_loss, last_metrics\n",
    "\n",
    "def plot_metrics(history, save_path):\n",
    "    \"\"\"\n",
    "    Generates and saves 3 plots:\n",
    "    1. Training vs Validation Loss\n",
    "    2. Global Error Metrics (RMSE, MAE)\n",
    "    3. R2 Score Distribution (10th, 50th, 90th percentiles)\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
    "\n",
    "    # 1. Losses\n",
    "    axs[0].plot(epochs, history['train_loss'], label=\"Train Loss\", color='blue')\n",
    "    axs[0].plot(epochs, history['val_loss'], label=\"Val Loss\", color='red')\n",
    "    axs[0].set_ylabel(\"Huber Loss\"); axs[0].legend(); axs[0].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[0].set_title(\"Training & Validation Loss\")\n",
    "\n",
    "    # 2. RMSE & MAE\n",
    "    axs[1].plot(epochs, history['RMSE'], label=\"RMSE\", color='orange')\n",
    "    axs[1].plot(epochs, history['MAE'], label=\"MAE\", color='green')\n",
    "    axs[1].set_ylabel(\"Error Units\"); axs[1].legend(); axs[1].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[1].set_title(\"Global Error Metrics\")\n",
    "\n",
    "    # 3. R2 Percentiles\n",
    "    axs[2].plot(epochs, history['R2_90PCT'], label=\"R2 Best (90%)\", linestyle='--', color='purple')\n",
    "    axs[2].plot(epochs, history['R2_50PCT'], label=\"R2 Median (50%)\", linewidth=2, color='black')\n",
    "    axs[2].plot(epochs, history['R2_10PCT'], label=\"R2 Worst (10%)\", linestyle=':', color='brown')\n",
    "    axs[2].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    axs[2].set_ylabel(\"R2 Score\"); axs[2].set_xlabel(\"Epochs\"); axs[2].legend(loc='lower right')\n",
    "    axs[2].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[2].set_title(\"R2 Score Distribution\")\n",
    "\n",
    "    plt.tight_layout(); plt.savefig(save_path); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4zuOOwW8RYw"
   },
   "source": [
    "### Execute Data Loading\n",
    "\n",
    "We execute the data loading in a standalone cell because the reconstruction of graph objects from the raw dataset is a time-consuming process. By isolating this step, we can load the data once and reuse the `train_loader` and `val_loader` objects for multiple training runs or hyperparameter adjustments without incurring the overhead of reloading the dataset each time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257305,
     "status": "ok",
     "timestamp": 1765992512030,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "-z0jtiHk8THb",
    "outputId": "a42eef24-1625-4eda-a5aa-e34c37c828d2"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "train_loader, val_loader, train_graphs, val_graphs = load_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rd5KuZHn8Wrr"
   },
   "source": [
    "### Model Initialization & Training Execution\n",
    "\n",
    "This cell orchestrates the complete training pipeline:\n",
    "\n",
    "1.  **Model Instantiation:** We create an instance of the `ClofNetModel` using the hyperparameters defined in the configuration and move it to the active device (GPU).\n",
    "2.  **Optimizer Setup:** We initialize the **Adam optimizer**, which updates the model parameters based on computed gradients, using the specified learning rate and weight decay (L2 regularization).\n",
    "3.  **Training Loop:** The `train_model` function executes the training process over the defined number of epochs. It returns the full training history, the best validation loss recorded, and the final performance metrics.\n",
    "4.  **Visualization:** Finally, `plot_metrics` generates the learning curves, providing a visual assessment of convergence and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 449385,
     "status": "ok",
     "timestamp": 1765992961420,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "mibv9MQz8OJV",
    "outputId": "3fe5dc60-b96e-4d9d-b69f-affafc9c3d8d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Model Instantiation\n",
    "model = create_model(config)\n",
    "\n",
    "# 2. Optimizer Setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "# 3. Training\n",
    "history, best_loss, last_metrics = train_model(model, train_loader, val_loader, optimizer, config)\n",
    "\n",
    "# 4. Curves/Plots\n",
    "plot_metrics(history, config.TRAINING_CURVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsEzGQ_c8ZOP"
   },
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egpwZ7LS9sTM"
   },
   "source": [
    "### Qualitative Analysis: 3D Stress Field Visualization\n",
    "\n",
    "This cell performs the final visual inspection of the model's predictions. We iterate through the specific test cases identified earlier: **Best, Median, and Worst** performance.\n",
    "\n",
    "**Key Feature: Independent Color Scaling**\n",
    "The `visualize_independent_scales` function allows the color map to scale dynamically for each plot:\n",
    "* **Left (Target):** Ground truth from FEM.\n",
    "* **Right (Prediction):** GNN output.\n",
    "\n",
    "**Why independent scales?**\n",
    "This allows us to verify if the model has learned the correct **stress distribution patterns** (topology of the hotspots), even if the absolute **magnitudes** are underestimated (a common issue known as \"over-smoothing\" in regression tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765992961437,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "xTaBXU_J9i-m",
    "outputId": "1750862c-80bd-4bb3-c47c-2531cd9988b1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. function for structure visualization  (AUTO-SCALE) ---\n",
    "def visualize_independent_scales(target_data, pred_data, original_pos, index):\n",
    "    \"\"\"\n",
    "    Displays Ground Truth (left) and Prediction (right) side-by-side.\n",
    "    EACH GRAPH HAS ITS OWN INDIVIDUAL COLOR SCALE.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for faces\n",
    "    if not hasattr(target_data, 'face') or target_data.face is None:\n",
    "        print(\"⚠️ No faces detected.\")\n",
    "        return\n",
    "\n",
    "    # --- Extract Geometry ---\n",
    "    x = original_pos[:, 0].cpu().numpy()\n",
    "    y = original_pos[:, 1].cpu().numpy()\n",
    "    z = original_pos[:, 2].cpu().numpy()\n",
    "\n",
    "    faces = target_data.face.cpu().numpy()\n",
    "    i, j, k = faces[0], faces[1], faces[2]\n",
    "\n",
    "    # --- Extract Values ---\n",
    "    val_target = target_data.y[:, 0].cpu().numpy()\n",
    "    val_pred = pred_data.y[:, 0].cpu().numpy()\n",
    "\n",
    "    # --- Calculate Individual Min/Max ---\n",
    "    # For Target (Ground Truth)\n",
    "    target_min, target_max = val_target.min(), val_target.max()\n",
    "\n",
    "    # For Prediction\n",
    "    pred_min, pred_max = val_pred.min(), val_pred.max()\n",
    "\n",
    "    # --- Create Figure ---\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{'type': 'scene'}, {'type': 'scene'}]],\n",
    "        subplot_titles=(\n",
    "            f\"GROUND TRUTH (Max: {target_max:.2f})\",\n",
    "            f\"PREDICTION (Max: {pred_max:.2f})\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Trace 1: Target (Own Scale) ---\n",
    "    fig.add_trace(\n",
    "        go.Mesh3d(\n",
    "            x=x, y=y, z=z, i=i, j=j, k=k,\n",
    "            intensity=val_target,\n",
    "            intensitymode='vertex',\n",
    "            colorscale='Jet',\n",
    "            cmin=target_min, cmax=target_max, # Local scale\n",
    "            opacity=1.0,\n",
    "            name='Target',\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title=\"Target Stress\",\n",
    "                x=0.45,       # Positioned between the two graphs\n",
    "                len=0.7,      # Bar length\n",
    "                thickness=15\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # --- Trace 2: Prediction (Own Scale) ---\n",
    "    fig.add_trace(\n",
    "        go.Mesh3d(\n",
    "            x=x, y=y, z=z, i=i, j=j, k=k,\n",
    "            intensity=val_pred,\n",
    "            intensitymode='vertex',\n",
    "            colorscale='Jet',\n",
    "            cmin=pred_min, cmax=pred_max, # Local scale\n",
    "            opacity=1.0,\n",
    "            name='Prediction',\n",
    "            showscale=True, # Show the second bar\n",
    "            colorbar=dict(\n",
    "                title=\"Pred. Stress\",\n",
    "                x=1.0,        # Positioned at the far right\n",
    "                len=0.7,\n",
    "                thickness=15\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # --- Layout ---\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Comparaison Case : Von Mises Stress Comparison - Independent Scales (Meshes {index})\",\n",
    "        height=600, width=1250, # Slightly wider for legends\n",
    "        margin=dict(r=20, b=0, l=0, t=50),\n",
    "        scene=dict(aspectmode='data'),\n",
    "        scene2=dict(aspectmode='data')\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# --- 2. EXECUTION ---\n",
    "INDEX = 5502\n",
    "\n",
    "print(f\"--- Independent scales visualization for part #{INDEX} ---\")\n",
    "\n",
    "# 1. Data\n",
    "target_graph = val_graphs[INDEX].clone().to(config.DEVICE)\n",
    "pos_orig = target_graph.pos.clone().cpu()\n",
    "\n",
    "# 2. Prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_tensor = model(target_graph.x, target_graph.pos, target_graph.edge_index, batch=None)\n",
    "\n",
    "# 3. Storage\n",
    "prediction_graph = target_graph.clone()\n",
    "prediction_graph.y = pred_tensor\n",
    "\n",
    "# 4. Display\n",
    "visualize_independent_scales(target_graph, prediction_graph, pos_orig, INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765992961444,
     "user": {
      "displayName": "Alexandre Vallet",
      "userId": "05101411858495561532"
     },
     "user_tz": -60
    },
    "id": "uICO6jyXQ9jk",
    "outputId": "2cf82875-6329-46d7-8eef-a34a3350f572"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. VISUALIZATION Structures (Independent Scales) ---\n",
    "def visualize_independent_scales(target_data, pred_data, original_pos, index):\n",
    "    \"\"\"\n",
    "    Displays Ground Truth (left) and Prediction (right) side-by-side.\n",
    "    EACH GRAPH HAS ITS OWN INDIVIDUAL COLOR SCALE.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for faces\n",
    "    if not hasattr(target_data, 'face') or target_data.face is None:\n",
    "        print(\"⚠️ No faces detected.\")\n",
    "        return\n",
    "\n",
    "    # --- Extract Geometry ---\n",
    "    x = original_pos[:, 0].cpu().numpy()\n",
    "    y = original_pos[:, 1].cpu().numpy()\n",
    "    z = original_pos[:, 2].cpu().numpy()\n",
    "\n",
    "    faces = target_data.face.cpu().numpy()\n",
    "    i, j, k = faces[0], faces[1], faces[2]\n",
    "\n",
    "    # --- Extract Values ---\n",
    "    val_target = target_data.y[:, 0].cpu().numpy()\n",
    "    val_pred = pred_data.y[:, 0].cpu().numpy()\n",
    "\n",
    "    # --- Calculate Individual Min/Max ---\n",
    "    # For Target (Ground Truth)\n",
    "    target_min, target_max = val_target.min(), val_target.max()\n",
    "\n",
    "    # For Prediction\n",
    "    pred_min, pred_max = val_pred.min(), val_pred.max()\n",
    "\n",
    "    # --- Create Figure ---\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{'type': 'scene'}, {'type': 'scene'}]],\n",
    "        subplot_titles=(\n",
    "            f\"GROUND TRUTH (Max: {target_max:.2f})\",\n",
    "            f\"PREDICTION (Max: {pred_max:.2f})\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Trace 1: Target (Own Scale) ---\n",
    "    fig.add_trace(\n",
    "        go.Mesh3d(\n",
    "            x=x, y=y, z=z, i=i, j=j, k=k,\n",
    "            intensity=val_target,\n",
    "            intensitymode='vertex',\n",
    "            colorscale='Jet',\n",
    "            cmin=target_min, cmax=target_max, # Local scale\n",
    "            opacity=1.0,\n",
    "            name='Target',\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title=\"Target Stress\",\n",
    "                x=0.45,       # Positioned between the two graphs\n",
    "                len=0.7,      # Bar length\n",
    "                thickness=15\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # --- Trace 2: Prediction (Own Scale) ---\n",
    "    fig.add_trace(\n",
    "        go.Mesh3d(\n",
    "            x=x, y=y, z=z, i=i, j=j, k=k,\n",
    "            intensity=val_pred,\n",
    "            intensitymode='vertex',\n",
    "            colorscale='Jet',\n",
    "            cmin=pred_min, cmax=pred_max, # Local scale\n",
    "            opacity=1.0,\n",
    "            name='Prediction',\n",
    "            showscale=True, # Show the second bar\n",
    "            colorbar=dict(\n",
    "                title=\"Pred. Stress\",\n",
    "                x=1.0,\n",
    "                len=0.7,\n",
    "                thickness=15\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # --- Layout ---\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Comparaison Case : Von Mises Stress Comparison - Independent Scales (Meshes {index})\",\n",
    "        height=600, width=1250,\n",
    "        margin=dict(r=20, b=0, l=0, t=50),\n",
    "        scene=dict(aspectmode='data'),\n",
    "        scene2=dict(aspectmode='data')\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# --- 2. EXECUTION ---\n",
    "INDEX = 5502 # piece index\n",
    "\n",
    "print(f\"--- Independent scales visualization for part #{INDEX} ---\")\n",
    "\n",
    "# 1. Data\n",
    "target_graph = val_graphs[INDEX].clone().to(config.DEVICE)\n",
    "pos_orig = target_graph.pos.clone().cpu()\n",
    "\n",
    "# 2. Prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_tensor = model(target_graph.x, target_graph.pos, target_graph.edge_index, batch=None)\n",
    "\n",
    "# 3. Storage\n",
    "prediction_graph = target_graph.clone()\n",
    "prediction_graph.y = pred_tensor\n",
    "\n",
    "# 4. Display\n",
    "visualize_independent_scales(target_graph, prediction_graph, pos_orig, INDEX)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMn176CSXGr0AUd+g4FOO2y",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkSbXnNnJ9uz"
      },
      "source": [
        "## Graph U-Net Training Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Drive Mount & Installation\n",
        "\n",
        "We mount Google Drive to access the data and install the required **PyTorch Geometric** libraries for our EGNN implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFJNM-mGAr4C",
        "outputId": "83f490f2-3589-46d0-a566-7da4b0fd5599"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.0+cu121.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We install the library specific for EGNN implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Librairie spÃ©cifique pour ton modÃ¨le EGNN\n",
        "!pip install egnn-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgYFSPfCKEl-"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-LkewsOKCBw",
        "outputId": "fcca3192-8f67-40e9-95f1-189e27e0f318"
      },
      "outputs": [],
      "source": [
        "%%writefile config.py\n",
        "\"\"\"Configuration and hyperparameters of the model\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class Config:\n",
        "    # ------------------------------------------------------------------\n",
        "    # Data (absolute paths)\n",
        "    # ------------------------------------------------------------------\n",
        "    TRAIN_DATA_PATH = (\n",
        "        \"/content/gdrive/MyDrive/DataSetML4/data/train_data.pt\"\n",
        "    )\n",
        "    VAL_DATA_PATH = (\n",
        "        \"/content/gdrive/MyDrive/DataSetML4/data/val_data.ptt\"\n",
        "    )\n",
        "\n",
        "    TRAIN_SUBSET_RATIO = 1\n",
        "    BATCH_SIZE = 2\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Architecture\n",
        "    # ------------------------------------------------------------------\n",
        "    IN_NODE_FEATURES = 9\n",
        "    HIDDEN_CHANNELS = 32\n",
        "    OUT_STRESS_DIM = 1\n",
        "    DEPTH = 4\n",
        "\n",
        "    LAMBDA_STRESS = 0.5\n",
        "\n",
        "    # EGNN physics\n",
        "    NUM_NEIGHBORS = 16\n",
        "    UPDATE_COORS = True\n",
        "    UPDATE_FEATS = True\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Training\n",
        "    # ------------------------------------------------------------------\n",
        "    NUM_EPOCHS = 10\n",
        "    LEARNING_RATE = 0.0005\n",
        "    WEIGHT_DECAY = 1e-16\n",
        "    GRADIENT_CLIP = 1.0\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Saving\n",
        "    # ------------------------------------------------------------------\n",
        "    BEST_MODEL_PATH = \"best_model.pt\"\n",
        "\n",
        "    TRAINING_CURVE_PATH = (\n",
        "        f\"training_curve_R{TRAIN_SUBSET_RATIO}_\"\n",
        "        f\"B{BATCH_SIZE}_\"\n",
        "        f\"H{HIDDEN_CHANNELS}_\"\n",
        "        f\"D{DEPTH}_\"\n",
        "        f\"E{NUM_EPOCHS}.png\"\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Device\n",
        "    # ------------------------------------------------------------------\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0detcbVcKGsB"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbpMOjRHKYKU",
        "outputId": "dfc28109-5d72-498c-aef3-8f2822c4fe07"
      },
      "outputs": [],
      "source": [
        "%%writefile utils.py\n",
        "\"\"\"Utils : normalizer and helpers.\"\"\"\n",
        "import torch\n",
        "\n",
        "class UnitGaussianNormalizer:\n",
        "    \"\"\"Gaussian Normalizer for the data\"\"\"\n",
        "\n",
        "    def __init__(self, x=None, eps=1e-5):\n",
        "        self.eps = eps\n",
        "        if x is not None:\n",
        "            self.mean = x.mean(dim=0, keepdim=True)\n",
        "            self.std = x.std(dim=0, keepdim=True) + eps\n",
        "        else:\n",
        "            self.mean = None\n",
        "            self.std = None\n",
        "\n",
        "    def encode(self, x):\n",
        "        if self.mean is None:\n",
        "            return x\n",
        "        return (x - self.mean) / self.std\n",
        "\n",
        "    def decode(self, x, sample_idx=None):\n",
        "        if self.mean is None:\n",
        "            return x\n",
        "        return x * self.std + self.mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QscJ-ELiKajU"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XscwI7xJKdgf",
        "outputId": "4d267187-dec0-4b47-d33e-541253986246"
      },
      "outputs": [],
      "source": [
        "%%writefile model.py\n",
        "\"\"\"Definition of the model architecture\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from egnn_pytorch import EGNN\n",
        "\n",
        "\n",
        "class EGNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    EGNN-based model for stress prediction on mesh.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the model using parameters from the configuration.\n",
        "\n",
        "        Args:\n",
        "            config: Configuration object containing architecture and\n",
        "                    EGNN hyperparameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Input embedding\n",
        "        # Maps raw node features to the hidden dimension\n",
        "        # ------------------------------------------------------------------\n",
        "        self.embedding = nn.Linear(\n",
        "            config.IN_NODE_FEATURES,\n",
        "            config.HIDDEN_CHANNELS\n",
        "        )\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Stress prediction head\n",
        "        # Predicts stress values per node from hidden features\n",
        "        # ------------------------------------------------------------------\n",
        "        self.stress_head = nn.Sequential(\n",
        "            nn.Linear(config.HIDDEN_CHANNELS, config.HIDDEN_CHANNELS),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.HIDDEN_CHANNELS, config.OUT_STRESS_DIM)\n",
        "        )\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # EGNN layers\n",
        "        # Stack of equivariant graph neural network blocks\n",
        "        # ------------------------------------------------------------------\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(config.DEPTH):\n",
        "            self.layers.append(\n",
        "                EGNN(\n",
        "                    dim=config.HIDDEN_CHANNELS,              # Node feature dimension\n",
        "                    m_dim=config.HIDDEN_CHANNELS,            # Message dimension\n",
        "                    num_nearest_neighbors=config.NUM_NEIGHBORS,\n",
        "                    update_coors=config.UPDATE_COORS,        # Whether to update coordinates\n",
        "                    update_feats=config.UPDATE_FEATS,        # Whether to update features\n",
        "                    norm_coors=True,                          # Normalize coordinate updates\n",
        "                    soft_edges=True                           # Use soft edge weights\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, h, pos, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            h (Tensor): Node features of shape (N, IN_NODE_FEATURES)\n",
        "            pos (Tensor): Node coordinates of shape (N, 3)\n",
        "            mask (Tensor, optional): Mask for valid nodes (for batching)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted stress values per node\n",
        "        \"\"\"\n",
        "\n",
        "        # Embed input node features into hidden space\n",
        "        h = self.embedding(h)\n",
        "\n",
        "        # Apply EGNN layers sequentially\n",
        "        for layer in self.layers:\n",
        "            h, pos = layer(h, pos, mask=mask)\n",
        "\n",
        "        # Predict stress from final node embeddings\n",
        "        pred_stress = self.stress_head(h)\n",
        "        return pred_stress\n",
        "\n",
        "\n",
        "def create_model(config):\n",
        "    \"\"\"\n",
        "    Factory function to create and move the model to the correct device.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing device information.\n",
        "\n",
        "    Returns:\n",
        "        EGNNModel: Model placed on the specified device.\n",
        "    \"\"\"\n",
        "    model = EGNNModel(config)\n",
        "    return model.to(config.DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyeqZ3MXKgo6"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i4uevrTKgHm",
        "outputId": "7ba17a0e-1e94-4920-9128-f2710943edf5"
      },
      "outputs": [],
      "source": [
        "%%writefile data_loading.py\n",
        "\"\"\"Loading and preparation of the data\"\"\"\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "import types\n",
        "from torch_geometric.data import Data, InMemoryDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from utils import UnitGaussianNormalizer\n",
        "\n",
        "\n",
        "sys.modules['utils'] = types.ModuleType('utils')\n",
        "sys.modules['utils'].UnitGaussianNormalizer = UnitGaussianNormalizer\n",
        "\n",
        "\n",
        "class TupleDataset(InMemoryDataset):\n",
        "    \"\"\"Dataset PyTorch Geometric from tuples(data, slices).\"\"\"\n",
        "\n",
        "    def __init__(self, data, slices):\n",
        "        super().__init__(None)\n",
        "        self.data = data\n",
        "        self.slices = slices\n",
        "\n",
        "    def len(self):\n",
        "        return self.data.num_graphs if hasattr(self.data, 'num_graphs') else self.slices['x'].size(0) - 1\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self.data.__class__.from_data_list([self.data])[idx]\n",
        "\n",
        "\n",
        "def get_graph_from_tuple(data, slices, idx):\n",
        "    \"\"\"Get's the idx_th tuple from data.\"\"\"\n",
        "    data_dict = {}\n",
        "    for key in slices.keys():\n",
        "        start, end = slices[key][idx].item(), slices[key][idx + 1].item()\n",
        "\n",
        "        if key in ['edge_index', 'face'] and data[key].dim() == 2:\n",
        "            data_dict[key] = data[key][:, start:end]\n",
        "        else:\n",
        "            data_dict[key] = data[key][start:end]\n",
        "\n",
        "    #Extract the position from the features\n",
        "    data_dict['pos'] = data_dict['x'][:, :3]\n",
        "    data_dict['x'] = data_dict['x'][:, 3:]\n",
        "\n",
        "    return Data(**data_dict)\n",
        "\n",
        "\n",
        "def load_data(config):\n",
        "    \"\"\"Loads training and validation data\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"DATA LOADING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    train_dataset_tuple = torch.load(config.TRAIN_DATA_PATH, weights_only=False)\n",
        "    train_data, train_slices = train_dataset_tuple\n",
        "\n",
        "    num_graphs = train_slices['x'].size(0) - 1\n",
        "    keep = int(num_graphs * config.TRAIN_SUBSET_RATIO)\n",
        "    print(f\"\\nðŸ“Š Dataset train: {num_graphs} graphs â†’ Kept: {keep} ({config.TRAIN_SUBSET_RATIO*100:.0f}%)\")\n",
        "\n",
        "    new_slices = {}\n",
        "    for key in train_slices.keys():\n",
        "        new_slices[key] = train_slices[key][:keep+1].clone()\n",
        "    train_slices = new_slices\n",
        "\n",
        "    test_dataset_tuple = torch.load(config.VAL_DATA_PATH, weights_only=False)\n",
        "    test_data, test_slices = test_dataset_tuple\n",
        "    print(f\"ðŸ“Š Dataset val: {test_slices['x'].size(0) - 1} graphs\")\n",
        "\n",
        "    # Create graohs\n",
        "    print(\"\\nðŸ”„ Graph extraction...\")\n",
        "    train_graphs = [get_graph_from_tuple(train_data, train_slices, i)\n",
        "                    for i in range(train_slices['x'].size(0) - 1)]\n",
        "    val_graphs = [get_graph_from_tuple(test_data, test_slices, i)\n",
        "                  for i in range(test_slices['x'].size(0) - 1)]\n",
        "\n",
        "    print(f\"âœ“ Train: {len(train_graphs)} graphs\")\n",
        "    print(f\"âœ“ Val: {len(val_graphs)} graphs\")\n",
        "\n",
        "    # CrÃ©er les dataloaders\n",
        "    train_loader = DataLoader(train_graphs, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_graphs, batch_size=config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(\"\\nâœ“ DataLoaders created\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return train_loader, val_loader, len(train_graphs), len(val_graphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrbiSJkfKnuw"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script trains and validates an EGNN-based model by converting batched graphs to dense form, computing Huber loss on valid nodes, tracking per-graph RÂ² distribution and global error metrics, saving the best model, and visualizing training dynamics over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBSA_0VRKnfS",
        "outputId": "618857ac-13fc-42b5-dd9f-97ca9c272f0b"
      },
      "outputs": [],
      "source": [
        "%%writefile training.py\n",
        "\"\"\"Training and validation loop.\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import time\n",
        "\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, device, gradient_clip, num_train_graphs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Convert sparse graph batch to dense tensors\n",
        "        h_dense, mask = to_dense_batch(batch.x, batch.batch)\n",
        "        pos_dense, _ = to_dense_batch(batch.pos, batch.batch)\n",
        "\n",
        "        # Forward pass\n",
        "        pred_stress_dense = model(h_dense, pos_dense, mask=mask)\n",
        "\n",
        "        # Remove padded nodes\n",
        "        pred_stress_valid = pred_stress_dense[mask].view(-1)\n",
        "        target_stress = batch.y.view(-1)\n",
        "\n",
        "        # Compute loss only on real (non-padded) nodes\n",
        "        loss = F.huber_loss(pred_stress_valid, target_stress, delta=0.5)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "    train_loss /= num_train_graphs\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def validate(model, val_loader, device, num_val_graphs):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        all_predictions_global = []\n",
        "        all_targets_global = []\n",
        "        r2_scores_per_graph = []\n",
        "\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Convert sparse graph batch to dense tensors\n",
        "            h_dense, mask = to_dense_batch(batch.x, batch.batch)\n",
        "            pos_dense, _ = to_dense_batch(batch.pos, batch.batch)\n",
        "\n",
        "            # Forward pass\n",
        "            pred_stress_dense = model(h_dense, pos_dense, mask=mask)\n",
        "\n",
        "            # Remove padded nodes\n",
        "            pred_stress_valid = pred_stress_dense[mask].view(-1)\n",
        "            target_stress = batch.y.view(-1)\n",
        "\n",
        "            # Compute loss only on real (non-padded) nodes\n",
        "            loss = F.huber_loss(pred_stress_valid, target_stress, delta=0.5)\n",
        "            val_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "            # Convert tensors to NumPy for metric computation\n",
        "            preds_np = pred_stress_valid.cpu().numpy()\n",
        "            targets_np = target_stress.view(-1).cpu().numpy()\n",
        "            batch_indices = batch.batch.cpu().numpy()\n",
        "\n",
        "            # Per-graph RÂ² computation\n",
        "            unique_graphs = np.unique(batch_indices)\n",
        "            for graph_id in unique_graphs:\n",
        "                # Select nodes belonging to the current graph\n",
        "                graph_mask = (batch_indices == graph_id)\n",
        "\n",
        "                graph_targets = targets_np[graph_mask]\n",
        "                graph_preds = preds_np[graph_mask]\n",
        "\n",
        "                if len(graph_targets) > 1:\n",
        "                    score = r2_score(graph_targets, graph_preds)\n",
        "                    r2_scores_per_graph.append(score)\n",
        "\n",
        "            all_predictions_global.append(pred_stress_valid)\n",
        "            all_targets_global.append(target_stress)\n",
        "\n",
        "    val_loss /= num_val_graphs\n",
        "\n",
        "    metrics = {}\n",
        "    metrics['all_r2'] = r2_scores_per_graph\n",
        "\n",
        "    if len(all_predictions_global) > 0:\n",
        "        final_preds = torch.cat(all_predictions_global).cpu().numpy()\n",
        "        final_targets = torch.cat(all_targets_global).cpu().numpy()\n",
        "\n",
        "        metrics['RMSE'] = np.sqrt(mean_squared_error(final_targets, final_preds))\n",
        "        metrics['MAE'] = mean_absolute_error(final_targets, final_preds)\n",
        "\n",
        "        if len(r2_scores_per_graph) > 0:\n",
        "            scores_array = np.array(r2_scores_per_graph)\n",
        "            metrics['R2_90PCT'] = np.percentile(scores_array, 90)\n",
        "            metrics['R2_50PCT'] = np.percentile(scores_array, 50)\n",
        "            metrics['R2_10PCT'] = np.percentile(scores_array, 10)\n",
        "        else:\n",
        "            metrics['R2_90PCT'] = 0\n",
        "            metrics['R2_50PCT'] = 0\n",
        "            metrics['R2_10PCT'] = 0\n",
        "\n",
        "    return val_loss, metrics\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, config, num_train_graphs, num_val_graphs):\n",
        "    print(\"=\" * 70)\n",
        "    print(\"START OF TRAINING\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'RMSE': [], 'MAE': [],\n",
        "        'R2_90PCT': [], 'R2_50PCT': [], 'R2_10PCT': []\n",
        "    }\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(1, config.NUM_EPOCHS + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = train_epoch(\n",
        "            model, train_loader, optimizer,\n",
        "            config.DEVICE, config.GRADIENT_CLIP, num_train_graphs\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss, metrics = validate(\n",
        "            model, val_loader, config.DEVICE, num_val_graphs\n",
        "        )\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Store metrics\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['RMSE'].append(metrics['RMSE'])\n",
        "        history['MAE'].append(metrics['MAE'])\n",
        "        history['R2_90PCT'].append(metrics['R2_90PCT'])\n",
        "        history['R2_50PCT'].append(metrics['R2_50PCT'])\n",
        "        history['R2_10PCT'].append(metrics['R2_10PCT'])\n",
        "\n",
        "        print(\n",
        "            f\"Ep {epoch:03d} | Val: {val_loss:.4f} | RMSE: {metrics['RMSE']:.3f} | \"\n",
        "            f\"R2(10/50/90): {metrics['R2_10PCT']:.2f} / {metrics['R2_50PCT']:.2f} | \"\n",
        "            f\"{metrics['R2_90PCT']:.2f} | Time: {time.time() - start_time}\"\n",
        "        )\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), config.BEST_MODEL_PATH)\n",
        "\n",
        "    return history, metrics\n",
        "\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"\n",
        "    Plot three figures: loss, error metrics (RMSE/MAE), and RÂ² percentiles.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    fig, axs = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
        "\n",
        "    # 1. Loss curves\n",
        "    axs[0].plot(epochs, history['train_loss'], label=\"Train Loss\", color='blue')\n",
        "    axs[0].plot(epochs, history['val_loss'], label=\"Validation Loss\", color='red')\n",
        "    axs[0].set_ylabel(\"Huber Loss\")\n",
        "    axs[0].set_title(\"Training & Validation Loss\")\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # 2. RMSE & MAE\n",
        "    axs[1].plot(epochs, history['RMSE'], label=\"RMSE\", color='orange')\n",
        "    axs[1].plot(epochs, history['MAE'], label=\"MAE\", color='green')\n",
        "    axs[1].set_ylabel(\"Error Units\")\n",
        "    axs[1].set_title(\"Global Error Metrics\")\n",
        "    axs[1].legend()\n",
        "    axs[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # 3. RÂ² percentiles (key paper metric)\n",
        "    axs[2].plot(epochs, history['R2_90PCT'], label=\"R2 Best (90%)\", linestyle='--', color='purple')\n",
        "    axs[2].plot(epochs, history['R2_50PCT'], label=\"R2 Median (50%)\", linewidth=2, color='black')\n",
        "    axs[2].plot(epochs, history['R2_10PCT'], label=\"R2 Worst (10%)\", linestyle=':', color='brown')\n",
        "\n",
        "    # Zero line to visualize negative scores\n",
        "    axs[2].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
        "\n",
        "    axs[2].set_ylabel(\"R2 Score\")\n",
        "    axs[2].set_xlabel(\"Epochs\")\n",
        "    axs[2].set_title(\"R2 Score Distribution (Per Geometry)\")\n",
        "    axs[2].legend(loc='lower right')\n",
        "    axs[2].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"Training curves saved to {save_path}\")\n",
        "\n",
        "\n",
        "def plot_losses(train_losses, val_losses, metrics, save_path):\n",
        "    \"\"\"\n",
        "    Plot and save Huber loss curves.\n",
        "    \"\"\"\n",
        "    best_val = min(val_losses)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label=\"Train Loss (Huber)\", linewidth=2)\n",
        "    plt.plot(val_losses, label=\"Validation Loss (Huber)\", linewidth=2)\n",
        "    plt.xlabel(\"Epoch\", fontsize=12)\n",
        "    plt.ylabel(\"Loss (Huber)\", fontsize=12)\n",
        "    plt.title(\"Training & Validation Loss\", fontsize=14)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.yscale('log')\n",
        "\n",
        "    # Add final metrics summary\n",
        "    if metrics is not None:\n",
        "        r2_90 = metrics.get('R2_90PCT', None)\n",
        "        r2_50 = metrics.get('R2_50PCT', None)\n",
        "        r2_10 = metrics.get('R2_10PCT', None)\n",
        "        rmse = metrics.get('RMSE', None)\n",
        "        mae = metrics.get('MAE', None)\n",
        "\n",
        "        def fmt(v):\n",
        "            return f\"{v:.4f}\" if isinstance(v, (int, float, np.floating)) else \"N/A\"\n",
        "\n",
        "        metrics_text = (\n",
        "            f\"R2_90: {fmt(r2_90)} | \"\n",
        "            f\"R2_50: {fmt(r2_50)} | \"\n",
        "            f\"R2_10: {fmt(r2_10)} | \"\n",
        "            f\"RMSE: {fmt(rmse)} | \"\n",
        "            f\"MAE: {fmt(mae)}\"\n",
        "        )\n",
        "\n",
        "        plt.figtext(0.5, -0.08, metrics_text, ha='center', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150)\n",
        "    print(f\"âœ“ Loss curve saved: {save_path}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWrv0wJIbAbq"
      },
      "source": [
        "### Execution - Data loading \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell initializes the configuration and loads the training and validation datasets into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "868804b4",
        "outputId": "c5e3c241-7b2b-4a38-9ea1-6569f272d7a5"
      },
      "outputs": [],
      "source": [
        "# --- CELL 1: Data loading (Run only once) ---\n",
        "import importlib\n",
        "import config\n",
        "from data_loading import load_data\n",
        "\n",
        "# Reload configuration to ensure the latest version is used\n",
        "importlib.reload(config)\n",
        "config = config.Config()\n",
        "\n",
        "# Load training and validation data\n",
        "train_loader, val_loader, num_train_graphs, num_val_graphs = load_data(config)\n",
        "\n",
        "print(\"\\nâœ… Data loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Execution - Training and validation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell initializes the model, executes the training and validation loops, computes evaluation metrics, and plots the resulting learning curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1682cefc",
        "outputId": "794032ca-86e3-42bc-f4c3-1d7a738bf6c9"
      },
      "outputs": [],
      "source": [
        "# --- CELL 2: Training (Can be re-run) ---\n",
        "import torch\n",
        "from model import create_model\n",
        "from training import train_model, plot_metrics  \n",
        "\n",
        "# 2. Model creation\n",
        "print(\"Model creation...\")\n",
        "model = create_model(config)\n",
        "\n",
        "# 3. Optimizer definition\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config.LEARNING_RATE,\n",
        "    weight_decay=config.WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# 4. Launch training\n",
        "print(\"Starting training...\")\n",
        "# Fix unpacking: train_model returns 2 values (training history and final metrics dictionary)\n",
        "history, metrics = train_model(\n",
        "    model, train_loader, val_loader, optimizer, config,\n",
        "    num_train_graphs, num_val_graphs\n",
        ")\n",
        "\n",
        "# Extract RÂ² values\n",
        "all_r2 = metrics['all_r2']\n",
        "\n",
        "# 5. Visualization\n",
        "# Use plot_metrics which matches the output of train_model\n",
        "plot_metrics(history, config.TRAINING_CURVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Qualitative Analysis: 3D Stress Field Visualization\n",
        "\n",
        "This cell performs the final visual inspection of the model's predictions. We iterate through the specific test cases identified earlier: **Best, Median, and Worst** performance.\n",
        "\n",
        "**Key Feature: Independent Color Scaling**\n",
        "The `visualize_side_by_side_independent` function allows the color map to scale dynamically for each plot:\n",
        "* **Left (Target):** Ground truth from FEM.\n",
        "* **Right (Prediction):** GNN output.\n",
        "\n",
        "**Why independent scales?**\n",
        "This allows us to verify if the model has learned the correct **stress distribution patterns** (topology of the hotspots), even if the absolute **magnitudes** are underestimated (a common issue known as \"over-smoothing\" in regression tasks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "wtZj9WkrrDa3",
        "outputId": "73b2d826-1080-4540-e633-7f2d1c9bf252"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "best_idx = np.argmax(all_r2)\n",
        "median_idx = np.argmin(np.abs(all_r2 - np.median(all_r2)))\n",
        "worst_idx = np.argmin(all_r2)\n",
        "\n",
        "val_graphs = val_loader.dataset\n",
        "\n",
        "# --- 1. ROBUST VISUALIZATION FUNCTION ---\n",
        "def visualize_results(target_data, pred_stress, original_pos, index):\n",
        "    \"\"\"\n",
        "    Displays ground truth and EGNN prediction side by side.\n",
        "    Automatically handles Mesh rendering (if faces are present) or Point Cloud rendering.\n",
        "    \"\"\"\n",
        "\n",
        "    # Move data to CPU\n",
        "    x = original_pos[:, 0].cpu().numpy()\n",
        "    y = original_pos[:, 1].cpu().numpy()\n",
        "    z = original_pos[:, 2].cpu().numpy()\n",
        "\n",
        "    # Ground truth and prediction\n",
        "    val_target = target_data.y.view(-1).cpu().numpy()\n",
        "    val_pred = pred_stress.view(-1).cpu().numpy()\n",
        "\n",
        "    # Common color scale\n",
        "    global_min = min(val_target.min(), val_pred.min())\n",
        "    global_max = max(val_target.max(), val_pred.max())\n",
        "\n",
        "    target_min, target_max = val_target.min(), val_target.max()\n",
        "    pred_min, pred_max = val_pred.min(), val_pred.max()\n",
        "\n",
        "    # Check: do we have faces for mesh rendering?\n",
        "    has_faces = hasattr(target_data, 'face') and target_data.face is not None\n",
        "\n",
        "    if has_faces:\n",
        "        faces = target_data.face.cpu().numpy()\n",
        "        i, j, k = faces[0], faces[1], faces[2]\n",
        "        trace_type = go.Mesh3d\n",
        "        kwargs = dict(i=i, j=j, k=k, intensitymode='vertex')\n",
        "    else:\n",
        "        print(\"â„¹ï¸ No faces detected, rendering as point cloud (Scatter3d).\")\n",
        "        trace_type = go.Scatter3d\n",
        "        kwargs = dict(mode='markers', marker=dict(size=4))\n",
        "\n",
        "    # --- Figure creation ---\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=2,\n",
        "        specs=[[{'type': 'scene'}, {'type': 'scene'}]],\n",
        "        subplot_titles=(f\"Target FEM (Graph {index})\", f\"EGNN Prediction (Graph {index})\")\n",
        "    )\n",
        "\n",
        "    # Trace 1: Ground truth\n",
        "    fig.add_trace(\n",
        "        trace_type(\n",
        "            x=x, y=y, z=z,\n",
        "            intensity=val_target,\n",
        "            colorscale='Jet',\n",
        "            cmin=target_min, cmax=target_max,\n",
        "            opacity=1.0,\n",
        "            name='Target',\n",
        "            colorbar=dict(\n",
        "                title=\"Target Stress\",\n",
        "                x=0.45,       # Positioned between the two plots\n",
        "                len=0.7,      # Colorbar length\n",
        "                thickness=15\n",
        "            ),\n",
        "            **kwargs\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Trace 2: Prediction\n",
        "    fig.add_trace(\n",
        "        trace_type(\n",
        "            x=x, y=y, z=z,\n",
        "            intensity=val_pred,\n",
        "            colorscale='Jet',\n",
        "            cmin=pred_min, cmax=pred_max,\n",
        "            opacity=1.0,\n",
        "            name='Prediction',\n",
        "            colorbar=dict(\n",
        "                title=\"Predicted Stress\",\n",
        "                x=1.0,        # Positioned on the far right\n",
        "                len=0.7,\n",
        "                thickness=15\n",
        "            ),\n",
        "            showscale=True,\n",
        "            **kwargs\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=f\"Von Mises Comparison â€“ Independent Scales (Mesh {index})\",\n",
        "        height=600,\n",
        "        width=1250,  # Slightly wider to accommodate colorbars\n",
        "        margin=dict(r=20, b=0, l=0, t=50),\n",
        "        scene=dict(aspectmode='data'),\n",
        "        scene2=dict(aspectmode='data')\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "# --- 2. EXECUTION ---\n",
        "# Choose a valid index (make sure it is < len(val_graphs))\n",
        "INDEX = best_idx\n",
        "print(all_r2[INDEX])\n",
        "\n",
        "print(f\"--- Preparing visualization for sample #{INDEX} ---\")\n",
        "\n",
        "# 1. Retrieve the graph\n",
        "target_graph = val_graphs[INDEX]\n",
        "\n",
        "# 2. Save original positions (CPU) for Plotly\n",
        "pos_orig = target_graph.pos.clone()\n",
        "\n",
        "# 3. Prepare data for the EGNN model\n",
        "# The model expects: [Batch, Nodes, Features], so we add a batch dimension (unsqueeze)\n",
        "h_input = target_graph.x.unsqueeze(0).to(config.DEVICE)     # Shape: [1, N, Features]\n",
        "pos_input = target_graph.pos.unsqueeze(0).to(config.DEVICE) # Shape: [1, N, 3]\n",
        "\n",
        "# Create a mask (all nodes are valid here, no padding)\n",
        "mask_input = torch.ones(\n",
        "    (1, target_graph.x.shape[0]),\n",
        "    dtype=torch.bool\n",
        ").to(config.DEVICE)\n",
        "\n",
        "# 4. Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Correct call according to your EGNNModel(h, pos, mask) definition\n",
        "    pred_dense = model(h_input, pos_input, mask=mask_input)\n",
        "\n",
        "    # Remove batch dimension and move back to CPU\n",
        "    pred_stress = pred_dense.squeeze(0).view(-1).cpu()\n",
        "\n",
        "# 5. Visualization\n",
        "visualize_results(target_graph, pred_stress, pos_orig, INDEX)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
